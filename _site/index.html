<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title></title>

  <!-- CSS -->
  <link rel="stylesheet" href="/ap/assets/css/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">
  
  <!-- Font Awesome -->
  <link rel="stylesheet" type="text/css" href="/ap/assets/css/fontawesome-all.min.css">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="16x16" href="/ap/assets/favicon.ico">

  <!-- Google Analytics -->
  

</head>


  <body>
    <nav class="nav">
      <div class="nav-container">
        <a href="/ap/">
          <h2 class="nav-title"></h2>
        </a>
        <!--ul>
          <li><a href="/ap/">About</a></li>
          <li><a href="/ap/portfolio/">Portfolio</a></li>
        </ul-->
    </div>
  </nav>

    <main>
      <div class="about">
  <div class="profile">
    <img class="selfie" alt="Rita Sevastjanova" src="assets/img/profile.jpg" />
    <div class="info">
      <div class="title">Rita Sevastjanova</div>
      <div class="lab">ETH Zurich, IVIA lab</div>
      <div class="description">I design interactive visualizations to support analysis of complex, high-dimensional data.</div>
      <div class="social-layer">
        <div class="social-icons">
          <ul>
            
<li>
  <a href="mailto:rita.sevastjanova@inf.ethz.ch" title="email">
    <span class="fa-stack fa-lg">
      <i class="fa fa-circle fa-stack-2x"></i>
      <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
    </span>
  </a>
</li>





















<li>
  <a href="https://www.linkedin.com/in/rita-sevastjanova-29a308149/" title="Follow On LinkedIn">
    <span class="fa-stack fa-lg">
      <i class="fa fa-circle fa-stack-2x"></i>
      <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
    </span>
  </a>
</li>

















<li>
  <a href="https://twitter.com/RSevastjanova" title="Follow On Twitter" class="type">
    <span class="fa-stack fa-lg">
      <i class="fa fa-circle fa-stack-2x"></i>
      <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
    </span>
  </a>
</li>








          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="content">
    <h2 id="about-me">About Me</h2>
<p>My research interests are related to large language models (LLMs) and their embedding spaces. Together with computational linguists and NLP researchers, I have worked on different projects that explored which linguistic properties and biases get encoded in contextual word embeddings. My focus lies on designing visual analytics approaches that support the interactive investigation of patterns in these high-dimensional vectors.</p>

    <h2>Selected Publications</h2>
    
    <div class="post">
      
      <div class="social-layer">
        <div class="social-icons">
          <ul>
            <section class="articles">
  
  <article>
    <div class="article-wrapper">
      <figure>
        <img src="/ap/assets/img/framework.png" alt="" width="50%"/>
      </figure>
      <div class="article-body">
        <h2>Conceptual Framework</h2>
        <p>
          What to consider when designing applications for word embedding contextualization tasks?
        </p>
        <a href="https://bib.dbvis.de/publications/view/1028"><button><i class="fa fa-file-pdf"></i></button></a>
        <a href="https://embedding-framework.lingvis.io"><button>DEMO</button></a>
      </div>
    </div>
  </article>
  
  <article>
    <div class="article-wrapper">
      <figure>
        <img src="/ap/assets/img/adapters.png" alt="" width="50%"/>
      </figure>
      <div class="article-body">
        <h2>Adapter Comparison</h2>
        <p>
          Which language models encode semantic concepts such as stereotypes or word sentiment?
        </p>
        <a href="https://ieeexplore.ieee.org/abstract/document/9904461"><button><i class="fa fa-file-pdf"></i></button></a>
        <a href="https://adapters.demo.lingvis.io"><button>DEMO</button></a>
      </div>
    </div>
  </article>
  
  <article>
    <div class="article-wrapper">
      <figure>
        <img src="/ap/assets/img/lmfingerprints.png" alt="" width="50%"/>
      </figure>
      <div class="article-body">
        <h2>Linguistic Properties</h2>
        <p>
          Which linguistic properties are encoded in embedding vectors in different model's layers?
        </p>
        <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14541"><button><i class="fa fa-file-pdf"></i></button></a>
        <a href="https://lmfingerprints.lingvis.io"><button>DEMO</button></a>
      </div>
    </div>
  </article>
  
  <article>
    <div class="article-wrapper">
      <figure>
        <img src="/ap/assets/img/function-words.png" alt="" width="50%"/>
      </figure>
      <div class="article-body">
        <h2>Impact of Function Words</h2>
        <p>
          Do models 'understand' the linguistic functionality of function words?
        </p>
        <a href="https://aclanthology.org/2022.coling-1.272/"><button><i class="fa fa-file-pdf"></i></button></a>
        <a href="https://function-words.lingvis.io"><button>DEMO</button></a>
      </div>
    </div>
  </article>
  
  <article>
    <div class="article-wrapper">
      <figure>
        <img src="/ap/assets/img/prompt-comparison.png" alt="" width="50%"/>
      </figure>
      <div class="article-body">
        <h2>Generated Text Comparison</h2>
        <p>
          How to effectively compare texts generated by two language models?
        </p>
        <a href="https://ieeexplore.ieee.org/abstract/document/10357717"><button><i class="fa fa-file-pdf"></i></button></a>
        <a href="https://prompt-comparison.lingvis.io"><button>DEMO</button></a>
      </div>
    </div>
  </article>
  
  <article>
    <div class="article-wrapper">
      <figure>
        <img src="/ap/assets/img/interlinked-projections.png" alt="" width="50%"/>
      </figure>
      <div class="article-body">
        <h2>Token Self-Similarity</h2>
        <p>
          What are the reasons for strong embedding contextualization?
        </p>
        <a href="https://aclanthology.org/2021.acl-long.39/"><button><i class="fa fa-file-pdf"></i></button></a>
        <a href="https://embeddings-explained.lingvis.io"><button>DEMO</button></a>
      </div>
    </div>
  </article>
  
</section>
          </ul>
        </div>
      </div>
      
      <!--
      <h2>Selected Publications</h2>
      <div class="">
        <ul>
            
  <li>
    <span><b>Rita Sevastjanova</b>, Simon Vogelbacher, Andreas Spitz, Daniel Keim, and Mennatallah El-Assady. 2023. Visual Comparison of Text Sequences Generated by Large Language Models. <i>In 2023 IEEE Visualization in Data Science (VDS), IEEE, 11-20.</i></span>
    <a href="https://ieeexplore.ieee.org/abstract/document/10357717" title="">
    <span class="fa-stack fa">
      <i class="fa fa-file-pdf"></i>
    </span>
    </a>
  </li>
  
  <li>
    <span><b>Rita Sevastjanova</b> and Mennatallah El-Assady. 2023. WEC-Explainer: A Descriptive Framework. <i>In Exploring Research Opportunities for Natural Language, Text, and Data Visualization (NLVIZ) Workshop at IEEE VIS.</i></span>
    <a href="https://bib.dbvis.de/publications/view/1028" title="">
    <span class="fa-stack fa">
      <i class="fa fa-file-pdf"></i>
    </span>
    </a>
  </li>
  
  <li>
    <span><b>Rita Sevastjanova</b>, A Kalouli, Christin Beck, Hanna Hauptmann, and Mennatallah El-Assady. 2022. LMFingerprints: Visual explanations of language model embedding spaces through layerwise contextualization scores. <i>In Computer Graphics Forum, 295-307.</i></span>
    <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14541" title="">
    <span class="fa-stack fa">
      <i class="fa fa-file-pdf"></i>
    </span>
    </a>
  </li>
  
  <li>
    <span><b>Rita Sevastjanova</b>, Eren Cakmak, Shauli Ravfogel, Ryan Cotterell, and Mennatallah El-Assady. 2022. Visual comparison of language model adaptation. <i>In IEEE Transactions on Visualization and Computer Graphics 29, 1, 1178-1188.</i></span>
    <a href="https://ieeexplore.ieee.org/abstract/document/9904461" title="">
    <span class="fa-stack fa">
      <i class="fa fa-file-pdf"></i>
    </span>
    </a>
  </li>
  
  <li>
    <span>Aikaterini-Lida Kalouli*, <b>Rita Sevastjanova</b>*, Christin Beck, and Maribel Romero. 2022. Negation, coordination, and quantifiers in contextualized language models. <i>In International Conference On Computational Linguistics (COLING)</i> (*equal contribution).</span>
    <a href="https://aclanthology.org/2022.coling-1.272/" title="">
    <span class="fa-stack fa">
      <i class="fa fa-file-pdf"></i>
    </span>
    </a>
  </li>
  
  <li>
    <span><b>Rita Sevastjanova</b> and Mennatallah El-Assady. 2022. Beware the Rationalization Trap! When Language Model Explainability diverges from our Mental Models of Language. <i>In Communication in Human-AI Interaction Workshop at IJCAI-ECAI'22.</i></span>
    <a href="https://bib.dbvis.de/publications/view/993" title="">
    <span class="fa-stack fa">
      <i class="fa fa-file-pdf"></i>
    </span>
    </a>
  </li>
  
  <li>
    <span><b>Rita Sevastjanova</b>, Aikaterini-Lida Kalouli, Christin Beck, Hanna Schäfer, and Mennatallah El-Assady. 2021. Explaining Contextualization in Language Models using Visual Analytics. <i>In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association for Computational Linguistics, 464-476.</i></span>
    <a href="https://aclanthology.org/2021.acl-long.39/" title="">
    <span class="fa-stack fa">
      <i class="fa fa-file-pdf"></i>
    </span>
    </a>
  </li>
  

        </ul>
      </div>
      -->
    </div>
    
    <h2>Exploring LLMs through Visualizations</h2>
    
    <div class="post">
      <div class="social-layer">
        <div class="social-icons">
          <ul>
            <section class="insights articles">
  
  <article>
    <div class="article-wrapper-insights">
        <h2>Pre-trained BERT learns gender bias</h2>
        <figure>
          <img src="/ap/assets/img/insights-bias.png" alt="" width="90%"/>
        </figure>
        <p>
          Visualizations can be used to compare models according to their word embedding spaces. By measuring the similarity between words in the model’s parameter space for specific concepts (e.g., stereotypical words and different gender), we can see associations that the models have learned from the training data.
        </p>
        <a href="https://ieeexplore.ieee.org/abstract/document/9904461"><button><i class="fa fa-file-pdf"></i></button></a>
        <a href="https://adapters.demo.lingvis.io"><button>DEMO</button></a>
    </div>
  </article>
  
  <article>
    <div class="article-wrapper-insights">
        <h2>BLOOM-1b3 learns unexpected biases</h2>
        <figure>
          <img src="/ap/assets/img/insights-prompt-bias4.png" alt="" width="90%"/>
        </figure>
        <p>
          Visualizations can also be used to compare model outputs for slightly varying prompts. By representing outputs in a common 2D space and using clustering on output trajectories, we visually group similar outputs for exploration. As showcased here, especially models with fewer parameters produce more biased outputs.
        </p>
        <a href="https://ieeexplore.ieee.org/abstract/document/10357717"><button><i class="fa fa-file-pdf"></i></button></a>
        <a href="https://prompt-comparison.lingvis.io"><button>DEMO</button></a>
    </div>
  </article>
  
  <article>
    <div class="article-wrapper-insights">
        <h2>Function word embeddings encode a lot of information</h2>
        <figure>
          <img src="/ap/assets/img/insights-lmfing-function-word2.png" alt="" width="90%"/>
        </figure>
        <p>
          In BERT, function word embeddings have a low self-similarity, especially in the upper layers. It means that the embedding vectors change if these words are used in different contexts. Hence, embedding vectors do not capture only the word semantic meaning. In upper layers, the nearest neighbors of function words do not necessary have the same POS tag, but rather the neighbors are content words from the same sentence.
        </p>
        <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14541"><button><i class="fa fa-file-pdf"></i></button></a>
        <a href="https://lmfingerprints.lingvis.io"><button>DEMO</button></a>
    </div>
  </article>
  
  <article>
    <div class="article-wrapper-insights">
        <h2>BERT does not understand function word meaning</h2>
        <figure>
          <img src="/ap/assets/img/insights-funct-words3.png" alt="" width="90%"/>
        </figure>
        <p>
          Although function words are highly contextualized, the model does not necessarily 'understand' their functional meaning in the sentence. In many cases, BERT makes semantically wrong predictions when varying used quantifiers, negations, and other function word classes.
        </p>
        <a href="https://aclanthology.org/2022.coling-1.272/"><button><i class="fa fa-file-pdf"></i></button></a>
        <a href="https://function-words.lingvis.io"><button>DEMO</button></a>
    </div>
  </article>
  
</section>
          </ul>
        </div>
      </div>
    </div>
    

  </div>
</div>

    </main>
    <footer>
      <span>
        &copy; <time datetime="2024-05-06 13:49:06 +0200">2024</time> Rita Sevastjanova. <a href="https://github.com/kssim/about-portfolio/">A.P</a> theme by kssim.
      </span>
    </footer>
  </body>
</html>
